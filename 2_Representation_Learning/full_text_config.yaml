# File Paths & Directories
data_dir: ./data/longeval_sci_training_2025_abstract
queries_file_name: "queries.txt"
qrels_file_name: "qrels.txt"
documents_dir_name: "documents"
output_dir: "output"

# --- Output file names ---
doc_embeddings_file_name: "doc_embeddings.index"
doc_ids_file_name: "doc_ids.pkl"
fine_tuned_model_dir_name: "fine_tuned_scibert"
run_file_name_template: "run_{snapshot_id}.txt" # For evaluation output

# Model Configuration
# model_name: "allenai/scibert_scivocab_uncased"
model_name_or_path: "allenai/scibert_scivocab_uncased" # Can be path to fine-tuned model
max_seq_length: 512 # 512 for standard SciBERT

# Hardware
device: "cuda" # cuda / cpu

# Data Processing
doc_content_fields: ["title", "abstract"] # Add "fulltext" if using full-text version
text_separator: " [SEP] " # Separator for concatenating title/abstract

# Embedding Configuration
embedding_batch_size: 64

# Indexing Configuration
faiss_index_type: "IndexFlatIP" # Inner Product for Cosine Similarity. Use IndexFlatL2 for Euclidean.
normalize_embeddings: true # Required for IndexFlatIP to represent Cosine Similarity

# Fine-tuning Configuration (using sentence-transformers)
# --- Set do_finetune to true to run fine-tuning ---
do_finetune: false
st_model_output_path: "output/fine_tuned_scibert" # Where to save the fine-tuned model
st_batch_size: 16
st_num_epochs: 1
st_learning_rate: 2.0e-5
st_loss: "MultipleNegativesRankingLoss" # Or TripletLoss, etc.
st_warmup_steps: 100
st_train_samples_max: -1 # Set to > 0 for quick testing, -1 for all data

# Retrieval Configuration
top_k: 100

# Evaluation Configuration
# --- List of snapshot IDs to simulate evaluation on ---
# --- Needs to be determined from data or organizers ---
evaluation_snapshots: ["2024-11", "2024-12"] # Example, replace with actual snapshot IDs
eval_metrics: ["ndcg@10", "map@100", "recall@100"]