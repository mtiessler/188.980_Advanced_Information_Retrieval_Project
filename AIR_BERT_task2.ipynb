{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies & Imports"
      ],
      "metadata": {
        "id": "fiVJiRbrYAQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies in correct order to avoid binary incompatibility\n",
        "!pip install numpy --quiet\n",
        "!pip install --upgrade pip --quiet\n",
        "!pip install torch transformers datasets --quiet\n",
        "!pip install scikit-learn --force-reinstall --no-deps --quiet\n",
        "!pip install ir_measures tqdm --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23XcKsldPG2",
        "outputId": "9d252d9e-8fc3-4b0f-cfe5-406758fc4fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m150.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m138.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m162.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [nvidia-cusolver-cu12]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ir_measures]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import collections\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DefaultDataCollator\n",
        ")\n",
        "from google.colab import drive\n",
        "import ir_measures\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "COLAB_DRIVE_ROOT_PATH = \"/content/drive/MyDrive/AIR_Project/\"\n",
        "DATA_DIR_NAME = \"longeval_sci_training_2025_abstract\"\n",
        "DATA_DIR = Path(COLAB_DRIVE_ROOT_PATH) / DATA_DIR_NAME\n",
        "QUERIES_FILE = DATA_DIR / \"queries.txt\"\n",
        "QRELS_FILE = DATA_DIR / \"qrels.txt\"\n",
        "DOCUMENTS_DIR = DATA_DIR / \"documents\"\n",
        "OUTPUT_DIR = Path(\"./longeval_output_colab_abstract_fp16\")\n",
        "PRETRAINED_MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
        "TREC_RUN_NAME = \"CLEF-Bert-Run-FP16\"\n",
        "NUM_TRAIN_EPOCHS = 5\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 128\n",
        "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_SEQ_LENGTH = 512\n",
        "EVAL_SPLIT_SIZE = 0.1\n",
        "# --- End Configuration Constants ---\n",
        "\n",
        "def mount_drive_and_verify_paths(data_dir_path, queries_file_path, qrels_file_path, docs_dir_path):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    paths_to_check = {\n",
        "        \"Dataset directory\": data_dir_path,\n",
        "        \"Queries file\": queries_file_path,\n",
        "        \"Qrels file\": qrels_file_path,\n",
        "        \"Documents directory\": docs_dir_path\n",
        "    }\n",
        "    all_exist = True\n",
        "    for name, path_val in paths_to_check.items():\n",
        "        if (name == \"Documents directory\" and not path_val.is_dir()) or \\\n",
        "           (name != \"Documents directory\" and not path_val.exists()):\n",
        "            print(f\"ERROR: {name} not found at: {path_val}\")\n",
        "            all_exist = False\n",
        "    if all_exist:\n",
        "        print(\"All required paths verified successfully.\")\n",
        "    return all_exist\n",
        "\n",
        "def load_queries(file_path):\n",
        "    queries = {}\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t', 1)\n",
        "                if len(parts) == 2:\n",
        "                    query_id, query_text = parts\n",
        "                    queries[query_id] = query_text\n",
        "                else:\n",
        "                    print(f\"WARNING: Skipping malformed line in queries file: {line.strip()}\")\n",
        "        print(f\"Loaded {len(queries)} queries.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading queries: {e}\")\n",
        "    return queries\n",
        "\n",
        "def load_qrels_for_ir_measures(file_path):\n",
        "    qrels_dict = collections.defaultdict(dict)\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 4:\n",
        "                    query_id, _, doc_id, relevance_score_str = parts\n",
        "                    qrels_dict[query_id][doc_id] = int(float(relevance_score_str))\n",
        "                else:\n",
        "                    print(f\"WARNING: Skipping malformed line in qrels file: {line.strip()}\")\n",
        "        print(f\"Loaded qrels for {len(qrels_dict)} queries for evaluation (scores as int).\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading qrels for evaluation: {e}\")\n",
        "    return qrels_dict\n",
        "\n",
        "def load_raw_qrels_data(file_path):\n",
        "    raw_qrels = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 4:\n",
        "                    query_id, _, doc_id, relevance_score = parts\n",
        "                    raw_qrels.append((query_id, doc_id, float(relevance_score)))\n",
        "                else:\n",
        "                    print(f\"WARNING: Skipping malformed line in qrels file: {line.strip()}\")\n",
        "        print(f\"Loaded {len(raw_qrels)} raw relevance judgments (scores as float).\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error loading raw qrels: {e}\")\n",
        "    return raw_qrels\n",
        "\n",
        "def load_documents_for_ids(docs_dir, required_doc_ids):\n",
        "    documents = {}\n",
        "    loaded_count = 0\n",
        "    if not required_doc_ids:\n",
        "        return documents\n",
        "\n",
        "    jsonl_files = list(docs_dir.glob('*.jsonl'))\n",
        "    total_files = len(jsonl_files)\n",
        "\n",
        "    for jsonl_file in tqdm(jsonl_files, total=total_files, desc=\"Scanning document files\"):\n",
        "        try:\n",
        "            with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        doc_data = json.loads(line)\n",
        "                        doc_id = str(doc_data.get(\"id\"))\n",
        "                        if doc_id in required_doc_ids and doc_id not in documents:\n",
        "                            title = doc_data.get(\"title\", \"\")\n",
        "                            abstract = doc_data.get(\"abstract\", \"\")\n",
        "                            documents[doc_id] = f\"{title} [SEP] {abstract}\".strip()\n",
        "                            loaded_count += 1\n",
        "                            if loaded_count == len(required_doc_ids):\n",
        "                                print(f\"Successfully loaded all {len(documents)} required documents.\")\n",
        "                                return documents\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "                    except Exception as e_doc:\n",
        "                        print(f\"WARNING: Error processing a document line in {jsonl_file}: {str(e_doc)}\")\n",
        "        except Exception as e_file:\n",
        "            print(f\"WARNING: Error opening or reading file {jsonl_file}: {str(e_file)}\")\n",
        "\n",
        "    if len(documents) < len(required_doc_ids):\n",
        "        print(f\"WARNING: Could only load {len(documents)} out of {len(required_doc_ids)} required documents.\")\n",
        "    else:\n",
        "        print(f\"Loaded {len(documents)} documents.\")\n",
        "    return documents\n",
        "\n",
        "def generate_trec_run_file(run_data, output_file, run_name):\n",
        "    with open(output_file, 'w') as f_out:\n",
        "        for q_id, doc_scores in run_data.items():\n",
        "            sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            for rank, (doc_id, score) in enumerate(sorted_docs, 1):\n",
        "                f_out.write(f\"{q_id} Q0 {doc_id} {rank} {score:.6f} {run_name}\\n\")\n",
        "    print(f\"TREC run file saved to {output_file}\")\n",
        "\n",
        "def load_initial_data(queries_fp, qrels_fp, docs_dp):\n",
        "    queries = load_queries(queries_fp)\n",
        "    raw_qrels_list = load_raw_qrels_data(qrels_fp)\n",
        "    qrels_for_eval_metrics = load_qrels_for_ir_measures(qrels_fp)\n",
        "\n",
        "    if not queries or not raw_qrels_list or not qrels_for_eval_metrics:\n",
        "        print(\"ERROR: Failed to load critical data (queries or qrels). Returning None for all.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    all_doc_ids_in_qrels = set(doc_id for _, doc_id, _ in raw_qrels_list)\n",
        "    documents_content = load_documents_for_ids(docs_dp, all_doc_ids_in_qrels)\n",
        "    if not documents_content and all_doc_ids_in_qrels:\n",
        "        print(\"ERROR: Failed to load required documents. Returning loaded data, but documents_content is None.\")\n",
        "        return queries, raw_qrels_list, qrels_for_eval_metrics, None\n",
        "\n",
        "    return queries, raw_qrels_list, qrels_for_eval_metrics, documents_content\n",
        "\n",
        "def prepare_and_split_dataset(\n",
        "    raw_qrels,\n",
        "    queries_map,\n",
        "    docs_map,\n",
        "    split_size\n",
        "):\n",
        "\n",
        "    dataset_items = []\n",
        "    for query_id, doc_id, relevance_score_float in raw_qrels:\n",
        "        query_text = queries_map.get(query_id)\n",
        "        doc_text = docs_map.get(doc_id)\n",
        "        if query_text and doc_text:\n",
        "            dataset_items.append({\n",
        "                \"query_id\": query_id, \"doc_id\": doc_id,\n",
        "                \"query_text\": query_text, \"document_text\": doc_text,\n",
        "                \"label\": relevance_score_float\n",
        "            })\n",
        "\n",
        "    if not dataset_items:\n",
        "        print(\"ERROR: No data items prepared after combining sources. Returning empty lists.\")\n",
        "        return [], []\n",
        "    print(f\"Prepared {len(dataset_items)} examples for model training/evaluation.\")\n",
        "\n",
        "    stratify_on = [item['query_id'] for item in dataset_items]\n",
        "    unique_query_ids = set(stratify_on)\n",
        "    stratify_param = stratify_on if len(unique_query_ids) > 1 and len(unique_query_ids) < len(dataset_items) else None\n",
        "\n",
        "    train_items, eval_items = train_test_split(\n",
        "        dataset_items, test_size=split_size, random_state=42, stratify=stratify_param\n",
        "    )\n",
        "    print(f\"Split data into {len(train_items)} training and {len(eval_items)} evaluation examples.\")\n",
        "    return train_items, eval_items\n",
        "\n",
        "def initialize_model_and_tokenizer(model_name):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error initializing model/tokenizer: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def tokenize_datasets(\n",
        "    train_items,\n",
        "    eval_items,\n",
        "    tokenizer\n",
        "):\n",
        "\n",
        "    _tokenize_fn = lambda examples: tokenizer(\n",
        "        examples[\"query_text\"], examples[\"document_text\"],\n",
        "        padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH\n",
        "    )\n",
        "\n",
        "    train_ds = Dataset.from_list(train_items)\n",
        "    tokenized_train = train_ds.map(\n",
        "        _tokenize_fn, batched=True,\n",
        "        remove_columns=[\"query_id\", \"doc_id\", \"query_text\", \"document_text\"]\n",
        "    ).rename_column(\"label\", \"labels\")\n",
        "\n",
        "    tokenized_eval = None\n",
        "    current_eval_items = list(eval_items)\n",
        "\n",
        "    if current_eval_items and len(current_eval_items) > 0:\n",
        "        eval_ds = Dataset.from_list(current_eval_items)\n",
        "        tokenized_eval = eval_ds.map(\n",
        "            _tokenize_fn, batched=True,\n",
        "            remove_columns=[\"query_id\", \"doc_id\", \"query_text\", \"document_text\"]\n",
        "        ).rename_column(\"label\", \"labels\")\n",
        "    else:\n",
        "        current_eval_items = []\n",
        "        print(\"WARNING: No evaluation data to tokenize for the trainer.\")\n",
        "\n",
        "    return tokenized_train, tokenized_eval, current_eval_items\n",
        "\n",
        "\n",
        "def train_model(model, train_data, eval_data, output_dp, training_config):\n",
        "    args = TrainingArguments(\n",
        "        output_dir=str(output_dp / \"training_results\"),\n",
        "        num_train_epochs=training_config['num_train_epochs'],\n",
        "        per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],\n",
        "        learning_rate=training_config['learning_rate'],\n",
        "        eval_strategy=\"epoch\" if eval_data and len(eval_data) > 0 else \"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=str(output_dp / 'logs'),\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True if eval_data and len(eval_data) > 0 else False,\n",
        "        metric_for_best_model=\"loss\" if eval_data and len(eval_data) > 0 else None,\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\",\n",
        "        fp16=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        optim=\"adamw_torch\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args, train_dataset=train_data,\n",
        "        eval_dataset=eval_data, data_collator=DefaultDataCollator()\n",
        "    )\n",
        "    print(\"Starting training...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"Training finished.\")\n",
        "        model_save_path = output_dp / \"final_model\"\n",
        "        trainer.save_model(model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"ERROR: Attempting to continue with evaluation.\")\n",
        "    return trainer\n",
        "\n",
        "def evaluate_model_ir(\n",
        "    trainer_instance,\n",
        "    eval_dataset_tokenized,\n",
        "    original_eval_items,\n",
        "    qrels_map,\n",
        "    output_dp,\n",
        "    run_name_prefix\n",
        "):\n",
        "    print(f\"DEBUG PRINT: original_eval_items contains {len(original_eval_items) if original_eval_items is not None else 'None'} items.\")\n",
        "    print(f\"DEBUG PRINT: eval_dataset_tokenized is {'present and has items' if eval_dataset_tokenized and len(eval_dataset_tokenized) > 0 else 'None or empty'}\")\n",
        "\n",
        "    if not original_eval_items:\n",
        "        print(\"DEBUG PRINT: Skipping IR evaluation because original_eval_items is empty or None.\")\n",
        "        return\n",
        "\n",
        "    print(\"DEBUG PRINT: Entered main else block for IR evaluation (original_eval_items is not empty).\")\n",
        "    trainer_instance.model.eval()\n",
        "\n",
        "    if not eval_dataset_tokenized or len(eval_dataset_tokenized) == 0:\n",
        "        print(\"DEBUG PRINT ERROR: Tokenized evaluation data for trainer is None or empty. Skipping IR prediction.\")\n",
        "        return\n",
        "\n",
        "    print(\"DEBUG PRINT: Proceeding with predictions using trainer.predict().\")\n",
        "    try:\n",
        "        predictions_output = trainer_instance.predict(eval_dataset_tokenized)\n",
        "        scores = predictions_output.predictions.squeeze(-1)\n",
        "\n",
        "        run_for_eval_metrics = collections.defaultdict(dict)\n",
        "        for i, item in enumerate(original_eval_items):\n",
        "            run_for_eval_metrics[item[\"query_id\"]][item[\"doc_id\"]] = scores[i].item()\n",
        "\n",
        "        eval_query_ids = set(item['query_id'] for item in original_eval_items)\n",
        "        filtered_qrels_for_eval = {\n",
        "            qid: docs for qid, docs in qrels_map.items() if qid in eval_query_ids\n",
        "        }\n",
        "\n",
        "        print(f\"DEBUG PRINT: run_for_eval_metrics has {len(run_for_eval_metrics)} queries.\")\n",
        "        print(f\"DEBUG PRINT: filtered_qrels_for_eval has {len(filtered_qrels_for_eval)} queries.\")\n",
        "\n",
        "        if run_for_eval_metrics and filtered_qrels_for_eval:\n",
        "            measures = [\n",
        "                ir_measures.nDCG@5, ir_measures.nDCG@10, ir_measures.nDCG@20,\n",
        "                ir_measures.P@5, ir_measures.P@10, ir_measures.P@20,\n",
        "                ir_measures.Recall@10, ir_measures.Recall@20, ir_measures.Recall@100,\n",
        "                ir_measures.MRR, ir_measures.MAP\n",
        "            ]\n",
        "            print(\"Calculating IR evaluation metrics with integer qrels...\")\n",
        "            results = ir_measures.calc_aggregate(measures, filtered_qrels_for_eval, run_for_eval_metrics)\n",
        "\n",
        "            metrics_file_path = output_dp / \"evaluation_metrics.txt\"\n",
        "            with open(metrics_file_path, 'w') as f:\n",
        "                f.write(\"IR EVALUATION METRICS\\n====================\\n\\n\")\n",
        "                for measure_obj, value in results.items():\n",
        "                    f.write(f\"{str(measure_obj)}: {value:.4f}\\n\")\n",
        "                    print(f\"{str(measure_obj)}: {value:.4f}\")\n",
        "            print(f\"Metrics saved to {metrics_file_path}\")\n",
        "\n",
        "            trec_run_file_path = output_dp / f\"{run_name_prefix}.txt\"\n",
        "            generate_trec_run_file(run_for_eval_metrics, trec_run_file_path, run_name_prefix)\n",
        "        else:\n",
        "            print(\"WARNING: DEBUG: Not enough data for IR metric calculation (run or qrels empty/mismatched after filtering).\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error during IR prediction or metric calculation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not mount_drive_and_verify_paths(DATA_DIR, QUERIES_FILE, QRELS_FILE, DOCUMENTS_DIR):\n",
        "        return\n",
        "\n",
        "    queries, raw_qrels, qrels_eval, documents = load_initial_data(QUERIES_FILE, QRELS_FILE, DOCUMENTS_DIR)\n",
        "    if not queries or not raw_qrels or not qrels_eval or not documents:\n",
        "        print(\"ERROR: Halting due to failure in initial data loading.\")\n",
        "        return\n",
        "\n",
        "    train_items, eval_items = prepare_and_split_dataset(raw_qrels, queries, documents, EVAL_SPLIT_SIZE)\n",
        "    if not train_items:\n",
        "        print(\"ERROR: No training items after split. Halting.\")\n",
        "        return\n",
        "\n",
        "    tokenizer, model = initialize_model_and_tokenizer(PRETRAINED_MODEL_NAME)\n",
        "    if not tokenizer or not model:\n",
        "        print(\"ERROR: Halting due to failure in model/tokenizer initialization.\")\n",
        "        return\n",
        "\n",
        "    tokenized_train, tokenized_eval, final_eval_items = tokenize_datasets(train_items, eval_items, tokenizer)\n",
        "\n",
        "    if not tokenized_train:\n",
        "        print(\"ERROR: Training data tokenization failed. Halting.\")\n",
        "        return\n",
        "\n",
        "    training_config_params = {\n",
        "        'num_train_epochs': NUM_TRAIN_EPOCHS,\n",
        "        'per_device_train_batch_size': PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        'per_device_eval_batch_size': PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "    }\n",
        "\n",
        "    trained_trainer = train_model(model, tokenized_train, tokenized_eval, OUTPUT_DIR, training_config_params)\n",
        "\n",
        "    if final_eval_items and len(final_eval_items) > 0 and tokenized_eval and len(tokenized_eval) > 0:\n",
        "         evaluate_model_ir(trained_trainer, tokenized_eval, final_eval_items, qrels_eval, OUTPUT_DIR, TREC_RUN_NAME)\n",
        "    else:\n",
        "        print(\"Skipping final IR evaluation as there are no evaluation items or tokenized evaluation data.\")\n",
        "\n",
        "    print(f\"All processing completed! Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dkpMs3UgBpK",
        "outputId": "b7eda1a7-2b20-4de7-adfa-f674f18dc5d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ERROR: Dataset directory not found at: /content/drive/MyDrive/AIR_Project/longeval_sci_training_2025_fulltext\n",
            "ERROR: Queries file not found at: /content/drive/MyDrive/AIR_Project/longeval_sci_training_2025_fulltext/queries.txt\n",
            "ERROR: Qrels file not found at: /content/drive/MyDrive/AIR_Project/longeval_sci_training_2025_fulltext/qrels.txt\n",
            "ERROR: Documents directory not found at: /content/drive/MyDrive/AIR_Project/longeval_sci_training_2025_fulltext/documents\n"
          ]
        }
      ]
    }
  ]
}