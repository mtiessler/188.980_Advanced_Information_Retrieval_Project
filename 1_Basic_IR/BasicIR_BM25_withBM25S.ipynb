{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba06607",
   "metadata": {},
   "source": [
    "another try with package - under construction\n",
    "\n",
    "rank_bm25\n",
    "maybe: \n",
    "bm25-pt\n",
    "https://github.com/jxmorris12/bm25_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d3caf",
   "metadata": {},
   "source": [
    "another try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83861a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "from rank_bm25 import BM25Okapi\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "#from tokenizers import Tokenizer\n",
    "\n",
    "import bm25s\n",
    "\n",
    "\n",
    "# Import nltk data\n",
    "# https://www.nltk.org/data.html\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')  # Ensure stopwords are downloaded\n",
    "\n",
    "\n",
    "# set data path\n",
    "\n",
    "data_path_abstract = r\"c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "\n",
    "# for dev\n",
    "#data_path_abstract = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\"\n",
    "\n",
    "\n",
    "data_path_abstract_q = r\"C:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\"\n",
    "\n",
    "# os.path.join(data_path_abstract_q, file_name)\n",
    "#data_folder = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c826496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install retriv\n",
    "# Note: SearchEngine is an alias for the SparseRetriever\n",
    "'''from retriv import SearchEngine\n",
    "\n",
    "collection = [\n",
    "  {\"id\": \"doc_1\", \"text\": \"Generals gathered in their masses\"},\n",
    "  {\"id\": \"doc_2\", \"text\": \"Just like witches at black masses\"},\n",
    "  {\"id\": \"doc_3\", \"text\": \"Evil minds that plot destruction\"},\n",
    "  {\"id\": \"doc_4\", \"text\": \"Sorcerer of death's construction\"},\n",
    "]\n",
    "\n",
    "se = SearchEngine(\"new-index\").index(collection)\n",
    "\n",
    "se.search(\"witches masses\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfbc75",
   "metadata": {},
   "source": [
    "load data, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1342512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "# one stream \n",
    "def load_and_preprocess_data(filepath):\n",
    "    def document_generator():\n",
    "        stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "        #stemmer = PorterStemmer()  # PorterStemmer 1/3 slower than SnowballStemmer\n",
    "        stemmer = SnowballStemmer('english')  # Initialize nltk stemmer\n",
    "\n",
    "        documents = []\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                doc = json.loads(line)\n",
    "                text = f\"{doc.get('title', '')} {doc.get('abstract', '')}\"\n",
    "                authors_text = ' '.join([author.get('name', '').lower() for author in doc.get('authors', [])])\n",
    "                text = f\"{text} {authors_text}\" \n",
    "                text = text.lower()\n",
    "                text = re.sub(r'\\W+', ' ', text)\n",
    "                tokens = word_tokenize(text)\n",
    "                #filtered_tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "                # Filter tokens: remove stop words, single characters, and numbers\n",
    "                filtered_tokens = [\n",
    "                    token for token in tokens \n",
    "                    if token not in stop_words and len(token) > 1 and not token.isdigit()\n",
    "                ]\n",
    "                # Apply steming\n",
    "                stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "                documents.append({\n",
    "                    'id': doc.get('id'),\n",
    "                    'tokens': stemmed_tokens\n",
    "                })\n",
    "    return document_generator()\n",
    "\n",
    "def load_and_preprocess_data_no_tokenization(filepath):\n",
    "    def document_generator():\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                doc = json.loads(line)\n",
    "                text = f\"{doc.get('title', '')} {doc.get('abstract', '')}\"\n",
    "                authors_text = ' '.join([author.get('name', '').lower() for author in doc.get('authors', [])])\n",
    "                text = f\"{text} {authors_text}\"\n",
    "                yield {\n",
    "                    'id': doc.get('id'),\n",
    "                    'text': text\n",
    "                }\n",
    "    return document_generator()\n",
    "\n",
    "# returns documents with tokens per stream (=field)  \n",
    "def load_and_preprocess_data_BM25F(filepath):\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    #stemmer = PorterStemmer()  # PorterStemmer 1/3 slower than SnowballStemmer\n",
    "    stemmer = SnowballStemmer('english')  # Initialize nltk stemmer\n",
    "\n",
    "    documents = []\n",
    "    #count_per_field = {'title': 0, 'abstract': 0, 'authors': 0}  # Initialize counts\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            doc = json.loads(line)\n",
    "            \n",
    "            # title tokenization\n",
    "            titel_text = doc.get('title', '').lower()       # convert to lowercase\n",
    "            titel_text = re.sub(r'\\W+', ' ', titel_text)    # replace non-word characters with space\n",
    "            title_tokens = word_tokenize(titel_text)\n",
    "            # Filter tokens: remove stop words, single characters, and numbers\n",
    "            title_tokens = [\n",
    "                token for token in title_tokens\n",
    "                if token not in stop_words and len(token) > 1 and not token.isdigit()\n",
    "            ]\n",
    "            title_tokens = [stemmer.stem(token) for token in title_tokens]\n",
    "\n",
    "            # abstract tokenization\n",
    "            abstract_text = doc.get('abstract', '') or ''  # convert missing to empty string\n",
    "            abstract_text = abstract_text.lower()\n",
    "            abstract_text = re.sub(r'\\W+', ' ', abstract_text)\n",
    "            abstract_tokens = word_tokenize(abstract_text)\n",
    "            # Filter tokens: remove stop words, single characters, and numbers\n",
    "            abstract_tokens = [\n",
    "                token for token in abstract_tokens\n",
    "                if token not in stop_words and len(token) > 1 and not token.isdigit()\n",
    "            ]\n",
    "            abstract_tokens = [stemmer.stem(token) for token in abstract_tokens]\n",
    "\n",
    "            # authors tokenization\n",
    "            # potential: optimize for names of persons\n",
    "            authors_text = ' '.join([author.get('name', '').lower() for author in doc.get('authors', [])])\n",
    "            authors_text = re.sub(r'\\W+', ' ', authors_text)\n",
    "            authors_tokens = word_tokenize(authors_text)\n",
    "            # Filter tokens: remove stop words, single characters, and numbers\n",
    "            authors_tokens = [\n",
    "                token for token in authors_tokens\n",
    "                if token not in stop_words and len(token) > 1 and not token.isdigit()\n",
    "            ]\n",
    "            authors_tokens = [stemmer.stem(token) for token in authors_tokens]\n",
    "            \n",
    "            # Combine all tokens\n",
    "            documents.append({\n",
    "                'id': doc.get('id'),\n",
    "                #'tokens': stemmed_tokens,\n",
    "                'title': title_tokens,\n",
    "                'abstract': abstract_tokens,\n",
    "                'authors': authors_tokens\n",
    "            })\n",
    "\n",
    "            # calculate number of tokens per field\n",
    "            #count_per_field['title'] += len(title_tokens)\n",
    "            #count_per_field['abstract'] += len(abstract_tokens)\n",
    "            #count_per_field['authors'] += len(authors_tokens)\n",
    "            \n",
    "    # number of documents\n",
    "    #doc_count = len(documents)\n",
    "    return documents #, doc_count, count_per_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec595ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process all files in a folder\n",
    "def load_and_preprocess_folder(folder_path):\n",
    "    def folder_generator():\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.jsonl'):\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                print(f\"Processing file: {filepath}\")\n",
    "                yield from load_and_preprocess_data(filepath)\n",
    "    return folder_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48564dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = r\"c\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "all_documents = load_and_preprocess_folder(data_path_abstract)\n",
    "#print(f\"Loaded {len(all_documents)} documents from folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390c0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\\documents_000001.jsonl\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23948/1037412976.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtokenized_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_documents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbm25L\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBM25L\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# takes lenght of documents into account\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23948/1037412976.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtokenized_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_documents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbm25L\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBM25L\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# takes lenght of documents into account\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23948/2785540539.py\u001b[0m in \u001b[0;36mfolder_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing file: {filepath}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfolder_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from rank_bm25 import BM25L\n",
    "# from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenized_docs = [doc['tokens'] for doc in all_documents]\n",
    "\n",
    "bm25L = BM25L(tokenized_docs) # takes lenght of documents into account\n",
    "#bm25 = BM25Okapi(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#query = \"retrieval quantum\"\n",
    "#tokenized_query = query.split(\" \")\n",
    "tokenized_query = [\"retrieval\", \"quantum\"]\n",
    "\n",
    "\n",
    "scoresBM25L = bm25L.get_scores(tokenized_query)\n",
    "#scoresBM25 = bm25.get_scores(tokenized_query)\n",
    "#print(scoresBM25L)\n",
    "\n",
    "ranked = bm25L.get_top_n(tokenized_query, all_documents, n=2)\n",
    "#r#anked_basic = bm25.get_top_n(tokenized_query, all_documents, n=2)\n",
    "\n",
    "print(ranked)\n",
    "#print(ranked_basic)\n",
    "\n",
    "ranked_doc_ids = [doc['id'] for doc in ranked]\n",
    "#ranked_doc_ids_basic = [doc['id'] for doc in ranked_basic]\n",
    "\n",
    "\n",
    "print(ranked_doc_ids)\n",
    "#print(ranked_doc_ids_basic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf2d86",
   "metadata": {},
   "source": [
    "# Evaluate \n",
    "\n",
    "- Search Information includes i) unique (anonymous) identifiers for individual user session; ii) search query; iii) returned results.\n",
    "- Click Information records, for each click, i) a unique (anonymous) identifier for individual user session; ii) the link that was clicked in the results list; iii) the position of clicked link in results list.\n",
    "\n",
    "queries:\n",
    "training queries\n",
    "│-- queries.txt # Tab-separated plain text file with queries and IDs \n",
    "- ID, search query\n",
    "\n",
    "qrels:\n",
    "│-- qrels.txt # Relevance judgments file in TREC format \n",
    "click information \n",
    "- ID, datum, dokumentID, relevanz\n",
    "\n",
    "(1) nDCG scores calculated on provided test sets. Such a classical evaluation measure is consistent with Web search, for which the discount emphasises the ordering of the top results.\n",
    "\n",
    "(2) Relative nDCG Drop (RnD) measured by computing the difference between snapshots test sets. This measure supports the evaluation of the impact of the data changes on the systems’ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Queries.txt\n",
    "def load_queries(filepath):\n",
    "    queries = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            query_text = parts[1]\n",
    "            queries[query_id] = query_text.split()  # Tokenize query\n",
    "    return queries\n",
    "\n",
    "\n",
    "\n",
    "# Parse qrels.txt\n",
    "def load_qrels(filepath):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance = int(parts[3])\n",
    "            qrels[query_id][doc_id] = relevance\n",
    "    return qrels\n",
    "\n",
    "# Load files\n",
    "data_path_queries = os.path.join(data_path_abstract_q, \"queries.txt\")\n",
    "data_path_qrels = os.path.join(data_path_abstract_q, \"qrels.txt\")\n",
    "\n",
    "queries = load_queries(data_path_queries)\n",
    "qrels = load_qrels(data_path_qrels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
