{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba06607",
   "metadata": {},
   "source": [
    "BM25S Approach\n",
    "\n",
    "https://github.com/xhluca/bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83861a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "#from tokenizers import Tokenizer\n",
    "\n",
    "import bm25s\n",
    "\n",
    "\n",
    "# Import nltk data\n",
    "# https://www.nltk.org/data.html\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')  # Ensure stopwords are downloaded\n",
    "\n",
    "\n",
    "# set data path\n",
    "\n",
    "data_path_abstract = r\"c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "\n",
    "# for dev\n",
    "#data_path_abstract = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\"\n",
    "\n",
    "\n",
    "data_path_abstract_q = r\"C:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\"\n",
    "\n",
    "# os.path.join(data_path_abstract_q, file_name)\n",
    "#data_folder = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95fe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "class FolderLoader:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        return FolderIterator(self.folder_path)\n",
    "\n",
    "\n",
    "class FolderIterator:\n",
    "    def __init__(self, folder_path):\n",
    "        self.filepaths = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith('.jsonl')\n",
    "        ]\n",
    "        self.file_index = 0\n",
    "        self.current_iterator = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while self.file_index < len(self.filepaths):\n",
    "            if self.current_iterator is None:\n",
    "                filepath = self.filepaths[self.file_index]\n",
    "                print(f\"Processing file: {filepath}\")\n",
    "                self.current_iterator = DocumentIterator(filepath)\n",
    "\n",
    "            try:\n",
    "                return next(self.current_iterator)\n",
    "            except StopIteration:\n",
    "                self.current_iterator = None\n",
    "                self.file_index += 1\n",
    "\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "class DocumentIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.file = open(filepath, 'r', encoding='utf-8')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        line = self.file.readline()\n",
    "        if not line:\n",
    "            self.file.close()\n",
    "            raise StopIteration\n",
    "\n",
    "        doc = json.loads(line)\n",
    "        text = f\"{doc.get('title', '')} {doc.get('abstract', '')}\"\n",
    "        authors_text = ' '.join([author.get('name', '').lower() for author in doc.get('authors', [])])\n",
    "        text = f\"{text} {authors_text}\"\n",
    "        id = doc.get('id')\n",
    "        #return text\n",
    "        # Return as a dictionary to preserver doc_id\n",
    "        return {\"id\": id, \"text\": text}\n",
    "            #'id': doc.get('id'),\n",
    "            #'text': text\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfbc75",
   "metadata": {},
   "source": [
    "load data, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48564dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = r\"c\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "all_documents = FolderLoader(data_path_abstract)\n",
    "#print(f\"Loaded {len(all_documents)} documents from folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390c0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000001.jsonl\n",
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000002.jsonl\n",
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000021.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects                              \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000001.jsonl\n",
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000002.jsonl\n",
      "Processing file: c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\\documents_000021.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding newlines for mmindex: 100%|██████████| 256M/256M [00:09<00:00, 27.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "# from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "import numpy as np\n",
    "import Stemmer\n",
    "\n",
    "#corpus = all_documents\n",
    "# get only text from dictionary\n",
    "corpus = [doc[\"text\"] for doc in all_documents]\n",
    "\n",
    "# optional: create a stemmer\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n",
    "\n",
    "# Create the BM25 model and index the corpus -> all_documents to preserve IDs\n",
    "retriever = bm25s.BM25(corpus=all_documents)\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "# save index with all_documts to preserve IDs\n",
    "retriever.save(\"bm25_index\", corpus=all_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f5d3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 (score: 5.91): ID: 1051705\n",
      "Rank 2 (score: 5.87): ID: 71415670\n",
      "[[{'id': '1051705', 'text': 'Covariant Hamiltonian formalisms for particles and antiparticles The hyperplane and proper time formalisms are discussed mainly for the\\nspin-half particles in the quantum case. A connection between these covariant\\nHamiltonian formalisms is established. It is showed that choosing the\\nspace-like hyperplanes instantaneously orthogonal to the direction of motion of\\nthe particle the proper time formalism is retrieved on the mass shell. As a\\nconsequence, the relation between the St\\\\\"uckelberg-Feynman picture and the\\nstandard canonical picture of quantum field theory is clarified.Comment: 19 pages, Latex, to be published in Int. J. Theor. Phy alvarez, edgardo t. garcia gaioli, fabian h.'}\n",
      "  {'id': '71415670', 'text': 'Frames and Phase Retrieval Phase retrieval tackles the problem of recovering a signal after loss of phase. The phase problem shows up in many different settings such as X-ray crystallography, speech recognition, quantum information theory, and coherent diffraction imaging. In this dissertation we present some results relating to three topics on phase retrieval. Chapters 1 and 2 contain the relevant background materials. In chapter 3, we introduce the notion of exact phase-retrievable frames as a way of measuring a frame\\\\u27s redundancy with respect to its phase retrieval property. We show that, in the d-dimensional real Hilbert space case, exact phase-retrievable frames can be of any lengths between 2d - 1 and d(d + 1)=2, inclusive. The complex Hilbert space case remains open. In chapter 4, we investigate phase-retrievability by studying maximal phase-retrievable subspaces with respect to a given frame. These maximal PR-subspaces can have different dimensions. We are able to identify the ones with the largest dimension and this can be considered as a generalization of the characterization of real phase-retrievable frames. In the basis case, we prove that if M is a k-dimensional PR-subspace then |supp(x)| ≥ k for every nonzero vector x 2 M. Moreover, if 1 ≤ k \\\\u3c [(d + 1)=2], then a k-dimensional PR-subspace is maximal if and only if there exists a vector x ϵ M such that |supp(x)| = k|. Chapter 5 is devoted to investigating phase-retrievable operator-valued frames. We obtain some characterizations of phase-retrievable frames for general operator systems acting on both finite and infinite dimensional Hilbert spaces; thus generalizing known results for vector-valued frames, fusion frames, and frames of Hermitian matrices. Finally, in Chapter 6, we consider the problem of characterizing projective representations that admit frame vectors with the maximal span property, a property that allows for an algebraic recovering of the phase-retrieval problem. We prove that every irreducible projective representation of a finite abelian group admits a frame vector with the maximal span property. All such vectors can be explicitly characterized. These generalize some of the recent results about phase-retrieval with Gabor (or STFT) measurements juste, ted'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#query = \"retrieval quantum\"\n",
    "#tokenized_query = query.split(\" \")\n",
    "retriever = bm25s.BM25.load(\"bm25_index\", mmap=True, load_corpus=True)\n",
    "\n",
    "query = \"retrieval quantum\"\n",
    "query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "\n",
    "results, scores = retriever.retrieve(query_tokens, k=2)\n",
    "\n",
    "for i in range(results.shape[1]):\n",
    "    doc, score = results[0, i], scores[0, i]\n",
    "   \n",
    "    print(f\"Rank {i+1} (score: {score:.2f}): ID: {doc['id']}\")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf2d86",
   "metadata": {},
   "source": [
    "# Evaluate \n",
    "\n",
    "- Search Information includes i) unique (anonymous) identifiers for individual user session; ii) search query; iii) returned results.\n",
    "- Click Information records, for each click, i) a unique (anonymous) identifier for individual user session; ii) the link that was clicked in the results list; iii) the position of clicked link in results list.\n",
    "\n",
    "queries:\n",
    "training queries\n",
    "│-- queries.txt # Tab-separated plain text file with queries and IDs \n",
    "- ID, search query\n",
    "\n",
    "qrels:\n",
    "│-- qrels.txt # Relevance judgments file in TREC format \n",
    "click information \n",
    "- ID, datum, dokumentID, relevanz\n",
    "\n",
    "(1) nDCG scores calculated on provided test sets. Such a classical evaluation measure is consistent with Web search, for which the discount emphasises the ordering of the top results.\n",
    "\n",
    "(2) Relative nDCG Drop (RnD) measured by computing the difference between snapshots test sets. This measure supports the evaluation of the impact of the data changes on the systems’ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Queries.txt\n",
    "def load_queries(filepath):\n",
    "    queries = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            query_text = parts[1]\n",
    "            queries[query_id] = query_text.split()  # Tokenize query\n",
    "    return queries\n",
    "\n",
    "\n",
    "\n",
    "# Parse qrels.txt\n",
    "def load_qrels(filepath):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance = int(parts[3])\n",
    "            qrels[query_id][doc_id] = relevance\n",
    "    return qrels\n",
    "\n",
    "# Load files\n",
    "data_path_queries = os.path.join(data_path_abstract_q, \"queries.txt\")\n",
    "data_path_qrels = os.path.join(data_path_abstract_q, \"qrels.txt\")\n",
    "\n",
    "queries = load_queries(data_path_queries)\n",
    "qrels = load_qrels(data_path_qrels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
