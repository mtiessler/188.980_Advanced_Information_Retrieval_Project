{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba06607",
   "metadata": {},
   "source": [
    "BM25S Approach\n",
    "\n",
    "https://github.com/xhluca/bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83861a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adrian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "#from tokenizers import Tokenizer\n",
    "\n",
    "import bm25s\n",
    "\n",
    "\n",
    "# Import nltk data\n",
    "# https://www.nltk.org/data.html\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')  # Ensure stopwords are downloaded\n",
    "\n",
    "\n",
    "# set data path\n",
    "\n",
    "data_path_abstract = r\"c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "\n",
    "# for dev\n",
    "#data_path_abstract = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\docShort\"\n",
    "\n",
    "\n",
    "data_path_abstract_q = r\"C:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\"\n",
    "\n",
    "# os.path.join(data_path_abstract_q, file_name)\n",
    "#data_folder = r\"c:\\Users\\hubin\\TULokal\\AIRLocal\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95fe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "class FolderLoader:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        return FolderIterator(self.folder_path)\n",
    "\n",
    "\n",
    "class FolderIterator:\n",
    "    def __init__(self, folder_path):\n",
    "        self.filepaths = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith('.jsonl')\n",
    "        ]\n",
    "        self.file_index = 0\n",
    "        self.current_iterator = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while self.file_index < len(self.filepaths):\n",
    "            if self.current_iterator is None:\n",
    "                filepath = self.filepaths[self.file_index]\n",
    "                print(f\"Processing file: {filepath}\")\n",
    "                self.current_iterator = DocumentIterator(filepath)\n",
    "\n",
    "            try:\n",
    "                return next(self.current_iterator)\n",
    "            except StopIteration:\n",
    "                self.current_iterator = None\n",
    "                self.file_index += 1\n",
    "\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "class DocumentIterator:\n",
    "    def __init__(self, filepath):\n",
    "        self.file = open(filepath, 'r', encoding='utf-8')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        line = self.file.readline()\n",
    "        if not line:\n",
    "            self.file.close()\n",
    "            raise StopIteration\n",
    "\n",
    "        doc = json.loads(line)\n",
    "        text = f\"{doc.get('title', '')} {doc.get('abstract', '')}\"\n",
    "        authors_text = ' '.join([author.get('name', '').lower() for author in doc.get('authors', [])])\n",
    "        text = f\"{text} {authors_text}\"\n",
    "        return text\n",
    "            #'id': doc.get('id'),\n",
    "            #'text': text\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfbc75",
   "metadata": {},
   "source": [
    "load data, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48564dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = r\"c\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\"\n",
    "all_documents = FolderLoader(data_path_abstract)\n",
    "#print(f\"Loaded {len(all_documents)} documents from folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390c0e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\\documents_000001.jsonl\n",
      "Processing file: c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\\documents_000021.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/114265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/114265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/114265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\\documents_000001.jsonl\n",
      "Processing file: c:\\Users\\Adrian\\Development\\air\\longeval_sci_training_2025_abstract\\longeval_sci_training_2025_abstract\\documents\\documents_000021.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f1c310e52b493db0c09efe670f9140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding newlines for mmindex:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from nltk.tokenize.destructive import NLTKWordTokenizer\n",
    "import numpy as np\n",
    "import Stemmer\n",
    "\n",
    "corpus = all_documents\n",
    "\n",
    "# optional: create a stemmer\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n",
    "\n",
    "# Create the BM25 model and index the corpus\n",
    "retriever = bm25s.BM25()\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "retriever.save(\"bm25_index\", corpus=corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f5d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 (score: 5.77): {'id': 109237, 'text': 'Covariant Hamiltonian formalisms for particles and antiparticles The hyperplane and proper time formalisms are discussed mainly for the\\nspin-half particles in the quantum case. A connection between these covariant\\nHamiltonian formalisms is established. It is showed that choosing the\\nspace-like hyperplanes instantaneously orthogonal to the direction of motion of\\nthe particle the proper time formalism is retrieved on the mass shell. As a\\nconsequence, the relation between the St\\\\\"uckelberg-Feynman picture and the\\nstandard canonical picture of quantum field theory is clarified.Comment: 19 pages, Latex, to be published in Int. J. Theor. Phy alvarez, edgardo t. garcia gaioli, fabian h.'}\n",
      "Rank 2 (score: 5.54): {'id': 109694, 'text': 'Ultrafast quantum random access memory utilizing single Rydberg atoms in\\n  a Bose-Einstein condensate We propose a long-lived and rapidly accessible quantum memory unit, for which\\nthe operational Hilbert space is spanned by states involving the two\\nmacroscopically occupied hyperfine levels of a miscible binary atomic\\nBose-Einstein condensate and the Rydberg state of a single atom. It is shown\\nthat an arbitrary qubit state, initially prepared using a flux qubit, can be\\nrapidly transferred to and from the trapped atomic ensemble in approximately 10\\nns and with a large fidelity of 97%, via an effective two-photon process using\\nan external laser for the transition to the Rydberg level. The achievable\\nultrafast transfer of quantum information therefore enables a large number of\\nstorage and retrieval cycles from the highly controllable quantum optics setup\\nof a dilute ultracold gas, even within the typically very short flux qubit\\nlifetimes of the order of microseconds.Comment: 5 pages of RevTex4-1, 2 figure fischer, uwe r. patton, kelly r.'}\n",
      "[[{'id': 109237, 'text': 'Covariant Hamiltonian formalisms for particles and antiparticles The hyperplane and proper time formalisms are discussed mainly for the\\nspin-half particles in the quantum case. A connection between these covariant\\nHamiltonian formalisms is established. It is showed that choosing the\\nspace-like hyperplanes instantaneously orthogonal to the direction of motion of\\nthe particle the proper time formalism is retrieved on the mass shell. As a\\nconsequence, the relation between the St\\\\\"uckelberg-Feynman picture and the\\nstandard canonical picture of quantum field theory is clarified.Comment: 19 pages, Latex, to be published in Int. J. Theor. Phy alvarez, edgardo t. garcia gaioli, fabian h.'}\n",
      "  {'id': 109694, 'text': 'Ultrafast quantum random access memory utilizing single Rydberg atoms in\\n  a Bose-Einstein condensate We propose a long-lived and rapidly accessible quantum memory unit, for which\\nthe operational Hilbert space is spanned by states involving the two\\nmacroscopically occupied hyperfine levels of a miscible binary atomic\\nBose-Einstein condensate and the Rydberg state of a single atom. It is shown\\nthat an arbitrary qubit state, initially prepared using a flux qubit, can be\\nrapidly transferred to and from the trapped atomic ensemble in approximately 10\\nns and with a large fidelity of 97%, via an effective two-photon process using\\nan external laser for the transition to the Rydberg level. The achievable\\nultrafast transfer of quantum information therefore enables a large number of\\nstorage and retrieval cycles from the highly controllable quantum optics setup\\nof a dilute ultracold gas, even within the typically very short flux qubit\\nlifetimes of the order of microseconds.Comment: 5 pages of RevTex4-1, 2 figure fischer, uwe r. patton, kelly r.'}]]\n"
     ]
    }
   ],
   "source": [
    "#query = \"retrieval quantum\"\n",
    "#tokenized_query = query.split(\" \")\n",
    "retriever = bm25s.BM25.load(\"bm25_index\", mmap=True, load_corpus=True)\n",
    "\n",
    "query = \"retrieval quantum\"\n",
    "query_tokens = bm25s.tokenize(query, stemmer=stemmer)\n",
    "\n",
    "results, scores = retriever.retrieve(query_tokens, k=2)\n",
    "\n",
    "for i in range(results.shape[1]):\n",
    "    doc, score = results[0, i], scores[0, i]\n",
    "    print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")\n",
    "\n",
    "print(results)\n",
    "\n",
    "# TODO: Fix IDs, currently IDs are generated by BM25S and are equal to line numbers (currently...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf2d86",
   "metadata": {},
   "source": [
    "# Evaluate \n",
    "\n",
    "- Search Information includes i) unique (anonymous) identifiers for individual user session; ii) search query; iii) returned results.\n",
    "- Click Information records, for each click, i) a unique (anonymous) identifier for individual user session; ii) the link that was clicked in the results list; iii) the position of clicked link in results list.\n",
    "\n",
    "queries:\n",
    "training queries\n",
    "│-- queries.txt # Tab-separated plain text file with queries and IDs \n",
    "- ID, search query\n",
    "\n",
    "qrels:\n",
    "│-- qrels.txt # Relevance judgments file in TREC format \n",
    "click information \n",
    "- ID, datum, dokumentID, relevanz\n",
    "\n",
    "(1) nDCG scores calculated on provided test sets. Such a classical evaluation measure is consistent with Web search, for which the discount emphasises the ordering of the top results.\n",
    "\n",
    "(2) Relative nDCG Drop (RnD) measured by computing the difference between snapshots test sets. This measure supports the evaluation of the impact of the data changes on the systems’ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Queries.txt\n",
    "def load_queries(filepath):\n",
    "    queries = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            query_text = parts[1]\n",
    "            queries[query_id] = query_text.split()  # Tokenize query\n",
    "    return queries\n",
    "\n",
    "\n",
    "\n",
    "# Parse qrels.txt\n",
    "def load_qrels(filepath):\n",
    "    qrels = defaultdict(dict)\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance = int(parts[3])\n",
    "            qrels[query_id][doc_id] = relevance\n",
    "    return qrels\n",
    "\n",
    "# Load files\n",
    "data_path_queries = os.path.join(data_path_abstract_q, \"queries.txt\")\n",
    "data_path_qrels = os.path.join(data_path_abstract_q, \"qrels.txt\")\n",
    "\n",
    "queries = load_queries(data_path_queries)\n",
    "qrels = load_qrels(data_path_qrels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
